---
title: "Capstone Interim Report"
author: "Paul Askew"
date: "29 May 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(NLP)
library(tm)
library(dplyr)
library(data.table)
```

## JHU Data Science Capstone Interim Report

The data files have already been downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip and the en_US options are being used for this project. The first step is to get an idea of the scale of the data so the files are summarised in the table below which gives the size, number of lines and the number of characters in the longest line of each file:

```{r file summary,echo=FALSE, warning=FALSE}

blogs <- readLines("data/en_US.blogs.txt",skipNul = TRUE)
news <- readLines("data/en_US.news.txt",skipNul = TRUE)
twitter <- readLines("data/en_US.twitter.txt",skipNul = TRUE)

require(data.table)
datasets <- c("blogs", "news", "twitter")
objSize <- sapply(datasets, function(x) {format(object.size(get(x)), units = "Mb")})
lines <- sapply(datasets, function(x) {length(get(x))})
chars <- c(max(nchar(blogs)),max(nchar(news)),max(nchar(twitter)))#sapply did not work correctly for this
overview <- data.table("Dataset" = datasets, "Object Size (Mb)" = objSize, "Lines" = lines,"Longest Line"=chars)
overview

```
## Data Cleaning

The data needs to be cleaned to remove things such as profanities, non text characters etc.  This is achieved using the tm package commonly used to support text mining [1].  We start by loading these in as a corpus of documents then the following transformations are implemented:

1. Switch to lower case
2. Remove profanities [2]
3. Remove numbers
4. Remove punctuation


This takes quite some time!

```{r cleaning echo=FALSE}

docs<-Corpus(DirSource("./data"))
docs <- tm_map(docs, content_transformer(tolower))
setInternet2(TRUE)
profanity_url <- "http://www.bannedwordlist.com/lists/swearWords.txt"
profanity <- scan(profanity_url, "")
docs <- tm_map(docs, removeWords, profanity)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
```



## N-grams

N-grams are the basis for language prediction algorithms being groups of commonly found words. A quick analysis of the corpus reveals the key words below in terms of most common words (unigrams), pairs fo words (bigrams) and groups fo three words (trigrams). Analysis could go further but it is felt at this stage that this is of limited benefit as it is likely that groups of 

```{r ngrams, echo=FALSE}
library(RWeka) # for generating N-grams

unigram_token <- function(x)
  NGramTokenizer(x, Weka_control(min = 1, max = 1))
bigram_token <- function(x)
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram_token <- function(x)
  NGramTokenizer(x, Weka_control(min = 3, max = 3))
fourgram_token <- function(x)
  NGramTokenizer(x, Weka_control(min = 4, max = 4))
fivegram_token <- function(x)
  NGramTokenizer(x, Weka_control(min = 5, max = 5))

unigram <- TermDocumentMatrix(docs, control=list(tokenize=unigram_token))
unigram_table <- findFreqTerms(unigram, lowfreq = 3)
unigram_table <- sort(rowSums(as.matrix(unigram[unigram_table,])), decreasing = TRUE)
unigram_table <- data.frame(unigram=names(unigram_table), frequency=unigram_table)

bigram <- TermDocumentMatrix(docs, control=list(tokenize=bigram_token))
bigram_table <- findFreqTerms(bigram, lowfreq = 3)
bigram_table <- sort(rowSums(as.matrix(bigram[bigram_table,])), decreasing = TRUE)
bigram_table <- data.frame(bigram=names(bigram_table), frequency=bigram_table)

trigram <- TermDocumentMatrix(docs, control=list(tokenize=trigram_token))
trigram_table <- findFreqTerms(trigram, lowfreq = 30)
trigram_table <- sort(rowSums(as.matrix(trigram[trigram_table,])), decreasing = TRUE)
trigram_table <- data.frame(trigram=names(trigram_table), frequency=trigram_table)
```

## Shiny App

The objective of this project is to create a Shiny App that will predict the next word based on the previously input word.  The captured n grams will be the basis of this and I will look to 

References

[1] http://onepager.togaware.com/TextMiningO.pdf

[2] http://www.bannedwordlist.com/lists/swearWords.txt
